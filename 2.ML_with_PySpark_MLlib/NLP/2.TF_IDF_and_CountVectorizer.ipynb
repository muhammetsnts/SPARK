{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2.TF_IDF_and_CountVectorizer.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPBsYyHSGtnk0sch0AahzG3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/muhammetsnts/SPARK/blob/main/2.ML_with_PySpark_MLlib/NLP/2.TF_IDF_and_CountVectorizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20vp4fpY7KZr"
      },
      "source": [
        "# Setup Environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NZZ4nklL2ja4"
      },
      "source": [
        "# install Java8\n",
        "!apt-get -q install openjdk-8-jdk-headless -qq > /dev/null\n",
        "\n",
        "# download spark3.1.1\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-3.1.1/spark-3.1.1-bin-hadoop2.7.tgz\n",
        "\n",
        "# unzip it\n",
        "!tar xf spark-3.1.1-bin-hadoop2.7.tgz\n",
        "\n",
        "# install findspark \n",
        "!pip install -q findspark\n",
        "\n",
        "\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"PATH\"] = os.environ[\"JAVA_HOME\"] + \"/bin:\" + os.environ[\"PATH\"]\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.1-bin-hadoop2.7\"\n",
        "\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "#spark = SparkSession.builder.appName('ops').getOrCreate()"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "weYpU4CD7Psu"
      },
      "source": [
        "# Hashing Term Frequency (HashingTF) and Inverse Document Frequency (IDF)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iLyYTwsk0bbo"
      },
      "source": [
        "# IDF: Inverse document frequency\n",
        "\n",
        "from pyspark.ml.feature import HashingTF, IDF, Tokenizer"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VNIDXevo03Ns"
      },
      "source": [
        "sentenceData = spark.createDataFrame([\n",
        "                                       (0.0, \"Hi, I heard about Saprk\"),\n",
        "                                       (0.0, \"I wish Java could use case classes\"),\n",
        "                                       (1.0, \"Logistic regression models are neat\")],\n",
        "                                       [\"label\", \"sentence\"])"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pYUC3NBy2OJ0",
        "outputId": "99336b88-e1ad-49d6-cd53-1b3959f25f9f"
      },
      "source": [
        "sentenceData.show()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+--------------------+\n",
            "|label|            sentence|\n",
            "+-----+--------------------+\n",
            "|  0.0|Hi, I heard about...|\n",
            "|  0.0|I wish Java could...|\n",
            "|  1.0|Logistic regressi...|\n",
            "+-----+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A9Mg-6xU2Zrh"
      },
      "source": [
        "tokenizer = Tokenizer(inputCol='sentence', outputCol=\"words\")"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MNLaRl-v2i5Z"
      },
      "source": [
        "words_data = tokenizer.transform(sentenceData)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l2wl-obD2q05",
        "outputId": "40e2ae75-c7d4-40f0-b21c-4da2db430547"
      },
      "source": [
        "words_data.show(truncate=False)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-----------------------------------+------------------------------------------+\n",
            "|label|sentence                           |words                                     |\n",
            "+-----+-----------------------------------+------------------------------------------+\n",
            "|0.0  |Hi, I heard about Saprk            |[hi,, i, heard, about, saprk]             |\n",
            "|0.0  |I wish Java could use case classes |[i, wish, java, could, use, case, classes]|\n",
            "|1.0  |Logistic regression models are neat|[logistic, regression, models, are, neat] |\n",
            "+-----+-----------------------------------+------------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fo-sHwb22xTB"
      },
      "source": [
        "hashing_tf = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\")"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NATEkRXE28ax"
      },
      "source": [
        "featured_data = hashing_tf.transform(words_data)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N8kJYiHx3EJI"
      },
      "source": [
        "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CSnzJHZg3KiB"
      },
      "source": [
        "idf_model = idf.fit(featured_data)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "niUz2SKP3W-I"
      },
      "source": [
        "rescaled_data = idf_model.transform(featured_data)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p6kgAAAJ3xHP",
        "outputId": "625eda4c-f358-4d80-f63f-0b4301b24b97"
      },
      "source": [
        "rescaled_data.select(\"label\", \"features\").show(truncate=False)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|label|features                                                                                                                                                                                      |\n",
            "+-----+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|0.0  |(262144,[18700,19036,40983,66273,233667],[0.6931471805599453,0.28768207245178085,0.6931471805599453,0.6931471805599453,0.6931471805599453])                                                   |\n",
            "|0.0  |(262144,[19036,20719,55551,58672,98717,109547,192310],[0.28768207245178085,0.6931471805599453,0.6931471805599453,0.6931471805599453,0.6931471805599453,0.6931471805599453,0.6931471805599453])|\n",
            "|1.0  |(262144,[46243,58267,91006,160975,190884],[0.6931471805599453,0.6931471805599453,0.6931471805599453,0.6931471805599453,0.6931471805599453])                                                   |\n",
            "+-----+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z_kParoA6teT"
      },
      "source": [
        "# Count Vectorization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uF032jHf35RQ"
      },
      "source": [
        "from pyspark.ml.feature import CountVectorizer"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UOURLtD44Im3"
      },
      "source": [
        "df = spark.createDataFrame([\n",
        "                            (0, \"a b c\".split(\" \")),\n",
        "                            (1, \"a b b c a\".split(\" \"))\n",
        "                           ], [\"id\", \"words\"])"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Ym5cbf24fnA",
        "outputId": "17a803b6-1220-4d58-f5e2-359a3e29bc00"
      },
      "source": [
        "df.show()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+---------------+\n",
            "| id|          words|\n",
            "+---+---------------+\n",
            "|  0|      [a, b, c]|\n",
            "|  1|[a, b, b, c, a]|\n",
            "+---+---------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aXZfP0Ut4gf3"
      },
      "source": [
        "# vocabSize : whats the max number of available vocabulary words I want\n",
        "# In this example a, b, c, so vocabSize = 3 \n",
        "\n",
        "# minDF : Affects that fitting process, by specifying the minimum number of documents a term must appearance to be included in the vocabulary.\n",
        "# So if minDF=2 and 1 term only appears on 1 document, and then minDF had that cut off then it wouldn't appear on count vectorizer  \n",
        "\n",
        "cv = CountVectorizer(inputCol=\"words\", outputCol = \"features\", vocabSize=3, minDF=2)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RdOF4qCu6jGB"
      },
      "source": [
        "model = cv.fit(df)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IK6Uo59s6m1-"
      },
      "source": [
        "result = model.transform(df)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "awunkvsS6pe2",
        "outputId": "7fbda188-3823-46cc-fa61-d6c1b5dcc7dd"
      },
      "source": [
        "result.show(truncate=False)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+---------------+-------------------------+\n",
            "|id |words          |features                 |\n",
            "+---+---------------+-------------------------+\n",
            "|0  |[a, b, c]      |(3,[0,1,2],[1.0,1.0,1.0])|\n",
            "|1  |[a, b, b, c, a]|(3,[0,1,2],[2.0,2.0,1.0])|\n",
            "+---+---------------+-------------------------+\n",
            "\n"
          ]
        }
      ]
    }
  ]
}