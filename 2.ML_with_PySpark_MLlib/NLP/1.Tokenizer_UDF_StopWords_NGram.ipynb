{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1.Tokenizer_UDF_StopWords_NGram.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyN+t9mY/g0LLxjMwR3gnQ2a",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/muhammetsnts/SPARK/blob/main/2.ML_with_PySpark_MLlib/NLP/1.Tokenizer_UDF_StopWords_NGram.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NZZ4nklL2ja4"
      },
      "source": [
        "# install Java8\n",
        "!apt-get -q install openjdk-8-jdk-headless -qq > /dev/null\n",
        "\n",
        "# download spark3.1.1\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-3.1.1/spark-3.1.1-bin-hadoop2.7.tgz\n",
        "\n",
        "# unzip it\n",
        "!tar xf spark-3.1.1-bin-hadoop2.7.tgz\n",
        "\n",
        "# install findspark \n",
        "!pip install -q findspark\n",
        "\n",
        "\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"PATH\"] = os.environ[\"JAVA_HOME\"] + \"/bin:\" + os.environ[\"PATH\"]\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.1-bin-hadoop2.7\"\n",
        "\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vfXalQBFdriq"
      },
      "source": [
        "from pyspark.ml.feature import Tokenizer, RegexTokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MaTmP9ABeJye"
      },
      "source": [
        "from pyspark.sql.functions import col, udf\n",
        "from pyspark.sql.types import IntegerType"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3JtbDR-YeV5V"
      },
      "source": [
        "sent_df = spark.createDataFrame([\n",
        "    (0, \"Hi I heard about Spark\"),\n",
        "    (1, \"I wish Java could use case classes\"),\n",
        "    (2, \"Logistic,regression,models,are,neat\")\n",
        "], [\"id\", \"sentence\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ml9UYO4YiGP5",
        "outputId": "32f08017-e4d4-4f74-8ca8-ec2eae334f77"
      },
      "source": [
        "sent_df.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+--------------------+\n",
            "| id|            sentence|\n",
            "+---+--------------------+\n",
            "|  0|Hi I heard about ...|\n",
            "|  1|I wish Java could...|\n",
            "|  2|Logistic,regressi...|\n",
            "+---+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oYA9zjiUktBt"
      },
      "source": [
        "# Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3StEBM8_iLdj"
      },
      "source": [
        "tokenizer = Tokenizer(inputCol='sentence', outputCol='words')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bz3EHLtfi4XC"
      },
      "source": [
        "# UDF - Lambda Expression\n",
        "We will define a UDF for counting the words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4etjCbRUioeQ"
      },
      "source": [
        "count_tokens = udf(lambda words: len(words), IntegerType())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qtdjQyFti3JI"
      },
      "source": [
        "tokenized = tokenizer.transform(sent_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fhmZpm-yjFDn",
        "outputId": "5627d71f-7653-4d1a-c322-849dc6e5b47a"
      },
      "source": [
        "tokenized.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+--------------------+--------------------+\n",
            "| id|            sentence|               words|\n",
            "+---+--------------------+--------------------+\n",
            "|  0|Hi I heard about ...|[hi, i, heard, ab...|\n",
            "|  1|I wish Java could...|[i, wish, java, c...|\n",
            "|  2|Logistic,regressi...|[logistic,regress...|\n",
            "+---+--------------------+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OLvW2VhGjkw3"
      },
      "source": [
        "After tokenization, we will count the words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XwQVrAR_jIDe",
        "outputId": "54c3316e-5e69-42c7-8fdc-81e844f8d8cb"
      },
      "source": [
        "tokenized.withColumn('tokens', count_tokens(col('words'))).show(truncate=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----------------------------------+------------------------------------------+------+\n",
            "|id |sentence                           |words                                     |tokens|\n",
            "+---+-----------------------------------+------------------------------------------+------+\n",
            "|0  |Hi I heard about Spark             |[hi, i, heard, about, spark]              |5     |\n",
            "|1  |I wish Java could use case classes |[i, wish, java, could, use, case, classes]|7     |\n",
            "|2  |Logistic,regression,models,are,neat|[logistic,regression,models,are,neat]     |1     |\n",
            "+---+-----------------------------------+------------------------------------------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yCfzawsaj0sc"
      },
      "source": [
        "In the row 2, there is only 1 token because there is no whitespace between the words."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jf0cMV0hkolV"
      },
      "source": [
        "# RegexTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hWf3FXjjiY3A"
      },
      "source": [
        "regex_tokenizer = RegexTokenizer(inputCol='sentence', outputCol='words', pattern='\\\\W')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yF4xGHmtjab3"
      },
      "source": [
        "regex_tokenized = regex_tokenizer.transform(sent_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "orX_e8_FkLYs",
        "outputId": "a9bd3c84-b513-4700-b8bf-9adc202bcbcb"
      },
      "source": [
        "regex_tokenized.withColumn('tokens', count_tokens(col('words'))).show(truncate=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----------------------------------+------------------------------------------+------+\n",
            "|id |sentence                           |words                                     |tokens|\n",
            "+---+-----------------------------------+------------------------------------------+------+\n",
            "|0  |Hi I heard about Spark             |[hi, i, heard, about, spark]              |5     |\n",
            "|1  |I wish Java could use case classes |[i, wish, java, could, use, case, classes]|7     |\n",
            "|2  |Logistic,regression,models,are,neat|[logistic, regression, models, are, neat] |5     |\n",
            "+---+-----------------------------------+------------------------------------------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hvv7LvsJkRmW"
      },
      "source": [
        "In this result we have the right token count for row #2. Because regex tokenizer counted the words according to commas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LfdRwQv4kwVz"
      },
      "source": [
        "# StopWordsRemover\n",
        "\n",
        "<p><code>StopWordsRemover</code> takes as input a sequence of strings (e.g. the output\n",
        "of a <a href=\"ml-features.html#tokenizer\">Tokenizer</a>) and drops all the stop\n",
        "words from the input sequences. The list of stopwords is specified by\n",
        "the <code>stopWords</code> parameter. Default stop words for some languages are accessible \n",
        "by calling <code>StopWordsRemover.loadDefaultStopWords(language)</code>, for which available \n",
        "options are &#8220;danish&#8221;, &#8220;dutch&#8221;, &#8220;english&#8221;, &#8220;finnish&#8221;, &#8220;french&#8221;, &#8220;german&#8221;, &#8220;hungarian&#8221;, \n",
        "&#8220;italian&#8221;, &#8220;norwegian&#8221;, &#8220;portuguese&#8221;, &#8220;russian&#8221;, &#8220;spanish&#8221;, &#8220;swedish&#8221; and &#8220;turkish&#8221;. \n",
        "A boolean parameter <code>caseSensitive</code> indicates if the matches should be case sensitive \n",
        "(false by default).</p>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cSqmmjIlkO3b"
      },
      "source": [
        "from pyspark.ml.feature import StopWordsRemover"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "525aRnpek601",
        "outputId": "23c98fd0-a93e-42c9-d4b9-c3031caecd84"
      },
      "source": [
        "sentenceData = spark.createDataFrame([\n",
        "                                  (0, [\"I\", \"saw\", \"the\", \"red\", \"balloon\"]),\n",
        "                                  (1, [\"Mary\", \"had\", \"a\", \"little\", \"lamb\"])\n",
        "                                  ], [\"id\", \"tokens\"])\n",
        "\n",
        "sentenceData.show(truncate=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----------------------------+\n",
            "|id |tokens                      |\n",
            "+---+----------------------------+\n",
            "|0  |[I, saw, the, red, balloon] |\n",
            "|1  |[Mary, had, a, little, lamb]|\n",
            "+---+----------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tNFkO1VamNIK",
        "outputId": "7fd6cf2e-adf6-4524-9149-c708ccbf6d48"
      },
      "source": [
        "remover = StopWordsRemover(inputCol='tokens', outputCol='filtered')  # we can add stopwords inside by using stopword parameter\n",
        "remover.transform(sentenceData).show(truncate=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----------------------------+--------------------+\n",
            "|id |tokens                      |filtered            |\n",
            "+---+----------------------------+--------------------+\n",
            "|0  |[I, saw, the, red, balloon] |[saw, red, balloon] |\n",
            "|1  |[Mary, had, a, little, lamb]|[Mary, little, lamb]|\n",
            "+---+----------------------------+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lehoHgzinPAP"
      },
      "source": [
        "# n-gram\n",
        "An n-gram is a sequence of nn tokens (typically words) for some integer nn. The NGram class can be used to transform input features into nn-grams.\n",
        "\n",
        "<p><code>NGram</code> takes as input a sequence of strings (e.g. the output of a <a href=\"ml-features.html#tokenizer\">Tokenizer</a>).  The parameter <code>n</code> is used to determine the number of terms in each $n$-gram. The output will consist of a sequence of $n$-grams where each $n$-gram is represented by a space-delimited string of $n$ consecutive words.  If the input sequence contains fewer than <code>n</code> strings, no output is produced.</p>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7-wtwyHtn1Lp"
      },
      "source": [
        "from pyspark.ml.feature import NGram"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kx7NTTshowHO"
      },
      "source": [
        "wordDataFrame = spark.createDataFrame([\n",
        "    (0, [\"Hi\", \"I\", \"heard\", \"about\", \"Spark\"]),\n",
        "    (1, [\"I\", \"wish\", \"Java\", \"could\", \"use\", \"case\", \"classes\"]),\n",
        "    (2, [\"Logistic\", \"regression\", \"models\", \"are\", \"neat\"])\n",
        "], [\"id\", \"words\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Si-VVytNo6We",
        "outputId": "28ca0e57-aecb-4544-930a-a7ee874452f1"
      },
      "source": [
        "ngram = NGram(n=2, inputCol='words', outputCol='grams')\n",
        "ngram.transform(wordDataFrame).show(truncate=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+------------------------------------------+------------------------------------------------------------------+\n",
            "|id |words                                     |grams                                                             |\n",
            "+---+------------------------------------------+------------------------------------------------------------------+\n",
            "|0  |[Hi, I, heard, about, Spark]              |[Hi I, I heard, heard about, about Spark]                         |\n",
            "|1  |[I, wish, Java, could, use, case, classes]|[I wish, wish Java, Java could, could use, use case, case classes]|\n",
            "|2  |[Logistic, regression, models, are, neat] |[Logistic regression, regression models, models are, are neat]    |\n",
            "+---+------------------------------------------+------------------------------------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5zPNY8gepHd-",
        "outputId": "2e28015b-1aa5-40b7-b9f0-b08cb585018d"
      },
      "source": [
        "ngram = NGram(n=3, inputCol='words', outputCol='grams')\n",
        "ngram.transform(wordDataFrame).show(truncate=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+------------------------------------------+--------------------------------------------------------------------------------+\n",
            "|id |words                                     |grams                                                                           |\n",
            "+---+------------------------------------------+--------------------------------------------------------------------------------+\n",
            "|0  |[Hi, I, heard, about, Spark]              |[Hi I heard, I heard about, heard about Spark]                                  |\n",
            "|1  |[I, wish, Java, could, use, case, classes]|[I wish Java, wish Java could, Java could use, could use case, use case classes]|\n",
            "|2  |[Logistic, regression, models, are, neat] |[Logistic regression models, regression models are, models are neat]            |\n",
            "+---+------------------------------------------+--------------------------------------------------------------------------------+\n",
            "\n"
          ]
        }
      ]
    }
  ]
}